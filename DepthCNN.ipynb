{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DepthCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaahirG/DepthDataApplication/blob/master/DepthCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNioIIK5r2uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "from google.colab import drive # https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "path = \"/content/drive/My Drive/DepthML/DepthConfirmed/*\"\n",
        "imageList = glob.glob(path)\n",
        "annotationMarker_Index = len(path) - 1   # -1 because list indexing starts at 0\n",
        "\n",
        "master_list = []\n",
        "\n",
        "for index,img in enumerate(imageList):\n",
        "    n = cv2.imread(img)\n",
        "    if n is None:\n",
        "      print(\"Problematic Image! SKIP and do not add to dataset.\")\n",
        "      continue\n",
        "    # Sort auto annotated image flags: 'n' , 'm' , 'D' from image filenames\n",
        "    # imageList[index] returns a string\n",
        "    label = imageList[index][annotationMarker_Index]\n",
        "\n",
        "    if label == 's': \n",
        "        master_list.append((n,2))\n",
        "    elif label == 'm':\n",
        "        master_list.append((n,1))\n",
        "    elif label == 'D': \n",
        "        master_list.append((n,0))\n",
        "    else: \n",
        "        print(\"error getting marker label\")\n",
        "    \n",
        "    print(label, n.shape, index)\n",
        "\n",
        "\n",
        "# For downsizing later\n",
        "scale_factor = 0.4 # of original size\n",
        "height = int(master_list[0][0].shape[0] * 0.4) #tuple[0].shape[0] == 480 height\n",
        "width = height\n",
        "dim = (width, height)\n",
        "\n",
        "print(master_list[0][1])\n",
        "# Randomize the order of images - replaces the actual list\n",
        "random.shuffle(master_list)\n",
        "print(master_list[0][1])\n",
        "shuffle_list = master_list\n",
        "\n",
        "\n",
        "print(len(master_list))\n",
        "print(dim)\n",
        "training_img = [] # training images\n",
        "training_labs = [] # training labels\n",
        "testing_img = []\n",
        "testing_labs = []\n",
        "# Splitting dataset into training and testing\n",
        "# image is tupl[0], corresponding label is tupl[1]\n",
        "# resize image even changes BGR values in the 3 channels to work with averaging multiple pixels for the resize of overall image\n",
        "# physically resizing image into smaller dimensions (not reshaping array)\n",
        "# reduced input layer(made up of each pixel) size by huge amount by resizing\n",
        "for index,tupl in enumerate(shuffle_list[:int(len(shuffle_list)*0.8)]):\n",
        "    print(index, tupl[1])\n",
        "    resize = cv2.resize(tupl[0], dim, interpolation = cv2.INTER_AREA)\n",
        "    training_img.append(resize)  # inter_area is best for downsizing\n",
        "    training_labs.append(tupl[1])\n",
        "\n",
        "# training and testing lists no have 80:20 split and the image vs labels are correctly corresponding index\n",
        "for testTup in shuffle_list[int(len(shuffle_list)*0.8):len(shuffle_list)]:\n",
        "    resize2 = cv2.resize(testTup[0], dim, interpolation = cv2.INTER_AREA)\n",
        "    testing_img.append(resize2)\n",
        "    testing_labs.append(testTup[1])\n",
        "\n",
        "# print(len(training_img))\n",
        "# print(len(training_labs))\n",
        "# print(len(testing_img))\n",
        "# print(len(testing_labs))\n",
        "\n",
        "# arrays have a shape attribute that returns a tuple of the length of each dimension of the array\n",
        "# 1 image = shape:(480,640,3), rows, cols, RGB per pixel\n",
        "\n",
        " # convert to numpy array for easier use with TF\n",
        "training_data = np.asarray(training_img)\n",
        "train_labels = np.asarray(training_labs) \n",
        "testing_data = np.asarray(testing_img) \n",
        "test_labels = np.asarray(testing_labs)\n",
        "\n",
        "training_img.clear()\n",
        "training_labs.clear()\n",
        "testing_img.clear()\n",
        "testing_labs.clear()\n",
        "print(len(testing_labs))\n",
        "\n",
        "classes = np.unique(train_labels) # array/list\n",
        "nClasses = len(classes)\n",
        "nRows,nCols,nDims = training_data.shape[1:] \n",
        "input_shape = (nRows, nCols, nDims)\n",
        "\n",
        "print(\"Image 0 Shape:\", training_data.shape[1:]) # should be resized\n",
        "print(\"SHAPE:\", nRows,nCols,nDims)\n",
        "\n",
        "# RESHAPE DATA STRUCTURE ALREADY EXISTS FROM NP.ARRAY CONVERSION\n",
        "# train_data = training_data.reshape(training_data.shape[0], nRows, nCols, nDims) # size, rows, cols, channels\n",
        "# test_data = testing_data.reshape(training_data.shape[0], nRows, nCols, nDims)\n",
        "\n",
        "# DATA STRUCTURE FROM RESHAPE\n",
        "# [ Encompassing numpy array\n",
        "#  [ 50k/Size/# of Total Images\n",
        "#   [ In Each of 50k: nRows Size\n",
        "#    [ In Each of nRowsSize: nColSize\n",
        "#     [ In Each of nColSize: nDimsSize\n",
        "     \n",
        "#     ] \n",
        "#    ]\n",
        "#   ]\n",
        "#  ]\n",
        "# ]\n",
        "\n",
        "# Change to float datatype\n",
        "train_data = training_data.astype('float32')\n",
        "test_data = testing_data.astype('float32')\n",
        "\n",
        "# Scale the data to lie between 0 to 1 - elementwise division of the whole array structure\n",
        "train_data /= 255 \n",
        "test_data /= 255 \n",
        "\n",
        "# Example showing floating, proper reshaping of arrays, and value for NN between 0&1\n",
        "print(\"RGB, PROPER RESHAPE AND SCALING:\", train_data[3][69][191])\n",
        "\n",
        "\n",
        "# USE CATEGORICAL CROSSENTROPY IF YOU ONE HOT, SPARSE IF THE CLASS IS JUST AN INTEGER.\n",
        "# From a representation like train_labels[0] = 2 --> array: [0. 0. 1.]\n",
        "# train_labels_one_hot = to_categorical(train_labels) \n",
        "# test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "def createModel():\n",
        "    model = Sequential()\n",
        "    # The first two layers with 32 filters of window size 3x3\n",
        "    # kernel/filter dot products the same size area and gets a value --> relu that value\n",
        "    # padding for some layers is 'same' beacuse it'll padd the same outer most adjacent value of the pixel\n",
        "    # spatial volume is decreasing as number of filters learned is increasing\n",
        "\n",
        "    # Stride size default: (1,1) start at cur, move over by 1 pixel\n",
        "    # Strides of 2Ã—2 can be a replacement to max pooling\n",
        "    # 5x5x3 for RGB channels is implied in kernel declaration size Conv2D\n",
        "    # stack all 32 or 64 etc feature maps together and that becomes the final output of the convolution layer\n",
        "    # applying the kernel leaves the size of image the same VISUALIZATION: https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37\n",
        "    model.add(Conv2D(32, (5, 5), padding='same', activation='relu', input_shape=input_shape)) \n",
        "    # with padding and kernel sliding, output size is the same as input: the padding takes care of the kernel sliding to remap the 1 pixel\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3))) # stride = 2 --> cut size to 1/3 --> # of total pixels / 9 --> gets a 3x3 space into 1 pixel then moves to next 3x3 space (no repeat because stride is 2)\n",
        "    model.add(Dropout(0.25)) # sets 25% of pixels to 0 to prevent overfitting\n",
        "\n",
        "    model.add(Conv2D(64, (5, 5), padding='same', activation='relu')) # NOTE CONVOLUTIONAL LAYERS WITH PADDING DON'T CHANGE SIZE - no matter size of the kernel (the padding 'same' accounts for that)\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2))) # 64x64x32 --> 4096 pixels / 4 --> 1024 --> 32x32x32\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Flatten the shrunken output which consists of one put together feature map of pixels\n",
        "    model.add(Flatten()) # flatten layer - input layer to FC NN\n",
        "    model.add(Dense(2048, activation='relu')) # number of hidden layer neurons should be half size of input layer.\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(nClasses, activation='softmax')) # probabilistic values between 0 and 1\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Keras preprocessing\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = createModel()\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "batch_size = 1 # split dataset into batches of 256 images for so thus more steps per epoch, but 10 batches of x not as cumbersome as 1 batch of x*10 for 1 step   \n",
        "epochs = 20\n",
        "\n",
        "# Keras API Documentation: https://keras.io/api/preprocessing/image/\n",
        "# Randomize actual image data for more robust feature extraction\n",
        "datagen = ImageDataGenerator(\n",
        "#         zoom_range=0.2, # randomly zoom into images\n",
        "#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDYmxZKJkeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model on the randomized batches generated by datagen.flow().\n",
        "# save to history_var (callback initiated each epoch that stores information about each epoch (dictionary)): https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
        "history_var = model.fit(datagen.flow(train_data, train_labels, batch_size=batch_size),\n",
        "                # number of total images / (# of images in a batch) = how many batches will have to pass through the CNN per epoch\n",
        "                steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))), # steps means all batches are evaluated in a single epoch, just not all at once, successively.\n",
        "                epochs=epochs,\n",
        "                validation_data=(test_data, test_labels),\n",
        "                verbose=1 # want training bar\n",
        "                )\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
        "\n",
        "\n",
        "# RESULT VISUALISATION\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(\"\\n\\nMODEL TEST SET LOSS: \", test_loss)\n",
        "print(\"MODEL TEST SET ACCURACY: \", test_accuracy)\n",
        "\n",
        "# plots of training set loss and accuracy vs testing validation set val_loss and accuracy\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history_var.history['loss'],'r',linewidth=3.0)\n",
        "plt.plot(history_var.history['val_loss'],'b',linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)\n",
        "\n",
        "\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history_var.history['accuracy'],'r',linewidth=3.0)\n",
        "plt.plot(history_var.history['val_accuracy'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)\n",
        "\n",
        "\n",
        "# User crosscheck - using model.predict()\n",
        "\n",
        "predictionList = []\n",
        "predictionPath = \"/content/drive/My Drive/DepthML/PredictionSamples/*\"\n",
        "filenameList = glob.glob(predictionPath)\n",
        "for filenames in filenameList:\n",
        "  image = cv2.resize(cv2.imread(filenames), dim, interpolation = cv2.INTER_AREA)\n",
        "  predictionList.append(image)\n",
        "\n",
        "plt.figure()\n",
        "figure, axesArray = plt.subplots(1, len(predictionList), figsize=(24, 5)) # subplot(row,col), figsize=(xnew,ynew)\n",
        "\n",
        "for index in range(0,len(predictionList)):\n",
        "  axesArray[index].imshow(predictionList[index])\n",
        "\n",
        "# convert to models required input format - same as above\n",
        "predictionList = np.asarray(predictionList)\n",
        "predictionList = predictionList.astype('float32')\n",
        "predictionList /= 255\n",
        "# print(\"IMAGE SHAPE CHECK: \", image.shape)  \n",
        "\n",
        "pred = model.predict(predictionList)\n",
        "for imgNum,confidence in enumerate(pred):\n",
        "  print(\"IMAGE NUMBER:\", imgNum, \"\\nConfidence:\", max(confidence))\n",
        "print(pred)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}